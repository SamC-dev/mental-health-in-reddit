{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9b77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load reddit authentications as environment variables from .env file\n",
    "load_dotenv(dotenv_path=\"../references/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RedditMentalHealthScraper:\n",
    "    def __init__(self, client_id: str, client_secret: str, user_agent: str):\n",
    "        \"\"\"\n",
    "        Initialize Reddit scraper\n",
    "        \n",
    "        Args:\n",
    "            client_id: Reddit API client ID\n",
    "            client_secret: Reddit API client secret  \n",
    "            user_agent: User agent string for API requests\n",
    "        \"\"\"\n",
    "        # api request\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "        \n",
    "        # Define rome timezone and period of analysis\n",
    "        rome_tz = pytz.timezone('Europe/Rome')\n",
    "        self.start_date = rome_tz.localize(datetime.datetime(2025, 1, 1, 0, 0, 0))\n",
    "        now_rome = datetime.datetime.now(rome_tz)\n",
    "        self.end_date = now_rome\n",
    "        \n",
    "        # Convert to UTC for Reddit API\n",
    "        self.start_timestamp = self.start_date.astimezone(pytz.UTC).timestamp()\n",
    "        self.end_timestamp = self.end_date.astimezone(pytz.UTC).timestamp()\n",
    "        \n",
    "        # Target subreddits and number of posts for each of them\n",
    "        self.subreddits = ['depression', 'depression_help', 'mentalhealth', 'Anxiety', 'Anxietyhelp']\n",
    "        self.posts_per_subreddit = 2000\n",
    "        \n",
    "    def get_post_data(self, post) -> Dict[str, Any]:\n",
    "        \"\"\"Extract only the essential data from a Reddit post\"\"\"\n",
    "        return {\n",
    "            'post_id': post.id,\n",
    "            'title': post.title,\n",
    "            'selftext': post.selftext,\n",
    "            'author': str(post.author) if post.author else '[deleted]',\n",
    "            'subreddit': str(post.subreddit),\n",
    "            'created_utc': post.created_utc,\n",
    "            'created_rome': datetime.datetime.fromtimestamp(\n",
    "                post.created_utc, pytz.timezone('Europe/Rome')\n",
    "            ).isoformat(),\n",
    "            'num_comments': post.num_comments,\n",
    "            'score': post.score,\n",
    "            'flair': post.link_flair_text\n",
    "        }\n",
    "    \n",
    "    def get_comment_data(self, comment, post_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract only the essential data from a Reddit comment\"\"\"\n",
    "        return {\n",
    "            'comment_id': comment.id,\n",
    "            'post_id': post_id,\n",
    "            'body': comment.body,\n",
    "            'author': str(comment.author) if comment.author else '[deleted]',\n",
    "            'created_utc': comment.created_utc,\n",
    "            'created_rome': datetime.datetime.fromtimestamp(\n",
    "                comment.created_utc, pytz.timezone('Europe/Rome')\n",
    "            ).isoformat(),\n",
    "            'score': comment.score,\n",
    "            'is_submitter': comment.is_submitter,\n",
    "            'depth': getattr(comment, 'depth', 0)\n",
    "        }\n",
    "    \n",
    "    def scrape_subreddit(self, subreddit_name: str) -> tuple[list[Dict], list[Dict]]:\n",
    "        \"\"\"\n",
    "        Scrape posts and comments from a specific subreddit using search + date filtering.\n",
    "     \"\"\"\n",
    "        logger.info(f\"Starting to scrape r/{subreddit_name}\")\n",
    "    \n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        posts_data = []\n",
    "        comments_data = []\n",
    "\n",
    "        try:\n",
    "            fetched_posts = []\n",
    "            # Fetch posts in reverse chronological order\n",
    "            # The limit is defined to 2000 but reddit max is 1000 anyway\n",
    "            for post in subreddit.new(limit = 2000):\n",
    "                if self.start_timestamp <= post.created_utc <= self.end_timestamp:\n",
    "                    fetched_posts.append(post)\n",
    "                elif post.created_utc < self.start_timestamp:\n",
    "                    break  \n",
    "                # time between requests to avoid overcharching reddit servers and be banned\n",
    "                # randomized to avoid being classified as bot\n",
    "                time.sleep(random.uniform(1.0, 2.0))  \n",
    "\n",
    "        except Exception as e:\n",
    "           logger.error(f\"Error searching posts from r/{subreddit_name}: {e}\")\n",
    "           return [], []\n",
    "\n",
    "        logger.info(f\"Found {len(fetched_posts)} posts in time range for r/{subreddit_name}\")\n",
    "\n",
    "        random.seed(77)\n",
    "\n",
    "        if len(fetched_posts) > self.posts_per_subreddit:\n",
    "           sampled_posts = random.sample(fetched_posts, self.posts_per_subreddit)\n",
    "        else:\n",
    "           sampled_posts = fetched_posts\n",
    "           logger.warning(f\"Only found {len(fetched_posts)} posts for r/{subreddit_name}, less than target {self.posts_per_subreddit}\")\n",
    "\n",
    "        for i, post in enumerate(sampled_posts):\n",
    "            try:\n",
    "                post_data = self.get_post_data(post)\n",
    "                posts_data.append(post_data)\n",
    "\n",
    "                # Fetch direct replies to the posts\n",
    "                post.comments.replace_more(limit=0) # \n",
    "\n",
    "                for top_level_comment in post.comments:\n",
    "                    if (\n",
    "                       hasattr(top_level_comment, 'body') and \n",
    "                       top_level_comment.body not in ['[deleted]', '[removed]']\n",
    "                    ):\n",
    "                      comment_data = self.get_comment_data(top_level_comment, post.id)\n",
    "                      comments_data.append(comment_data)\n",
    "\n",
    "                    # if you only want to take up to 10 top-level comments\n",
    "                    #if len(comments_data) >= 10:\n",
    "                     #   break  \n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    logger.info(f\"Processed {i + 1}/{len(sampled_posts)} posts from r/{subreddit_name}\")\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing post {post.id} from r/{subreddit_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Completed r/{subreddit_name}: {len(posts_data)} posts, {len(comments_data)} comments\")\n",
    "        return posts_data, comments_data\n",
    "    \n",
    "    def scrape_all_subreddits(self, output_dir: str = \"../references/data\"):\n",
    "        \"\"\"\n",
    "        Scrape all target subreddits and save data\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save the extracted data\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        all_posts = []\n",
    "        all_comments = []\n",
    "        \n",
    "        for subreddit_name in self.subreddits:\n",
    "            try:\n",
    "                posts_data, comments_data = self.scrape_subreddit(subreddit_name)\n",
    "                \n",
    "                # Save individual subreddit data\n",
    "                posts_df = pd.DataFrame(posts_data)\n",
    "                comments_df = pd.DataFrame(comments_data)\n",
    "                \n",
    "                posts_df.to_csv(f\"{output_dir}/{subreddit_name}_posts.csv\", index=False)\n",
    "                comments_df.to_csv(f\"{output_dir}/{subreddit_name}_comments.csv\", index=False)\n",
    "                \n",
    "                # Add to combined dataset\n",
    "                all_posts.extend(posts_data)\n",
    "                all_comments.extend(comments_data)\n",
    "                \n",
    "                logger.info(f\"Saved data for r/{subreddit_name}\")\n",
    "                \n",
    "                # Sleep between subreddits to be respectful to Reddit's servers\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to scrape r/{subreddit_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save combined dataset\n",
    "        all_posts_df = pd.DataFrame(all_posts)\n",
    "        all_comments_df = pd.DataFrame(all_comments)\n",
    "        \n",
    "        all_posts_df.to_csv(f\"{output_dir}/all_posts_combined.csv\", index=False)\n",
    "        all_comments_df.to_csv(f\"{output_dir}/all_comments_combined.csv\", index=False)\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'scrape_date': datetime.datetime.now().isoformat(),\n",
    "            'time_period_start': self.start_date.isoformat(),\n",
    "            'time_period_end': self.end_date.isoformat(),\n",
    "            'subreddits': self.subreddits,\n",
    "            'target_posts_per_subreddit': self.posts_per_subreddit,\n",
    "            'total_posts_collected': len(all_posts),\n",
    "            'total_comments_collected': len(all_comments),\n",
    "            'posts_per_subreddit': {sub: len([p for p in all_posts if p['subreddit'] == sub]) for sub in self.subreddits}\n",
    "        }\n",
    "        \n",
    "        with open(f\"{output_dir}/metadata.json\", 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Scraping completed! Total: {len(all_posts)} posts, {len(all_comments)} comments\")\n",
    "        logger.info(f\"Data saved to {output_dir}/\")\n",
    "        \n",
    "        return all_posts_df, all_comments_df\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # You need to register a Reddit app at https://www.reddit.com/prefs/apps/\n",
    "    # and get your credentials\n",
    "    CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "    CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "    USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = RedditMentalHealthScraper(\n",
    "        client_id=CLIENT_ID,\n",
    "        client_secret=CLIENT_SECRET,\n",
    "        user_agent=USER_AGENT\n",
    "    )\n",
    "    \n",
    "    # Run the scraping\n",
    "    posts_df, comments_df = scraper.scrape_all_subreddits()\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n=== SCRAPING SUMMARY ===\")\n",
    "    print(f\"Total posts collected: {len(posts_df)}\")\n",
    "    print(f\"Total comments collected: {len(comments_df)}\")\n",
    "    print(f\"\\nPosts per subreddit:\")\n",
    "    print(posts_df['subreddit'].value_counts())\n",
    "    print(f\"\\nComments per subreddit:\")\n",
    "    comment_subreddit_counts = comments_df.merge(\n",
    "        posts_df[['post_id', 'subreddit']], \n",
    "        on='post_id', \n",
    "        how='left'\n",
    "    )['subreddit'].value_counts()\n",
    "    print(comment_subreddit_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720973b4",
   "metadata": {},
   "source": [
    "The following summary has been preferred because the original output allowed identification of users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afca0b5",
   "metadata": {},
   "source": [
    "### SCRAPING SUMMARY\n",
    "\n",
    "Total posts collected: 4822\n",
    "\n",
    "Total comments collected: 11310\n",
    "\n",
    "#### Posts per subreddit:\n",
    "\n",
    "depression         983\n",
    "\n",
    "Anxietyhelp        974\n",
    "\n",
    "depression_help    968\n",
    "\n",
    "mentalhealth       961\n",
    "\n",
    "Anxiety            936\n",
    "\n",
    "Name: count, dtype: int64\n",
    "\n",
    "\n",
    "#### Comments per subreddit:\n",
    "\n",
    "Anxietyhelp        2799\n",
    "\n",
    "Anxiety            2739\n",
    "\n",
    "depression_help    2422\n",
    "\n",
    "mentalhealth       1972\n",
    "\n",
    "depression         1378\n",
    "\n",
    "Name: count, dtype: int64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
