{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a010b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk import punkt\n",
    "from langdetect import detect, DetectorFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\carru\\Desktop\\css project\\venv\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     c:\\Users\\carru\\Desktop\\css project\\venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk_data_path = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "nltk.download(\"punkt_tab\", download_dir=nltk_data_path)\n",
    "nltk.download(\"stopwords\", download_dir=nltk_data_path)\n",
    "\n",
    "data_folder_path = \"../references/data/\"\n",
    "cleaned_folder_path = \"../references/data_cleaned\"\n",
    "os.makedirs(cleaned_folder_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d04ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_comments_combined.csv\n",
      "\n",
      " Cleaning: all_comments_combined.csv\n",
      " Cleaned all_comments_combined.csv: 11310 → 10588 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_all_comments_combined.csv\n",
      "all_posts_combined.csv\n",
      "\n",
      " Cleaning: all_posts_combined.csv\n",
      " Cleaned all_posts_combined.csv: 4822 → 4650 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_all_posts_combined.csv\n",
      "Anxietyhelp_comments.csv\n",
      "\n",
      " Cleaning: Anxietyhelp_comments.csv\n",
      " Cleaned Anxietyhelp_comments.csv: 2799 → 2603 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_Anxietyhelp_comments.csv\n",
      "Anxietyhelp_posts.csv\n",
      "\n",
      " Cleaning: Anxietyhelp_posts.csv\n",
      " Cleaned Anxietyhelp_posts.csv: 974 → 872 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_Anxietyhelp_posts.csv\n",
      "Anxiety_comments.csv\n",
      "\n",
      " Cleaning: Anxiety_comments.csv\n",
      " Cleaned Anxiety_comments.csv: 2739 → 2571 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_Anxiety_comments.csv\n",
      "Anxiety_posts.csv\n",
      "\n",
      " Cleaning: Anxiety_posts.csv\n",
      " Cleaned Anxiety_posts.csv: 936 → 917 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_Anxiety_posts.csv\n",
      "depression_comments.csv\n",
      "\n",
      " Cleaning: depression_comments.csv\n",
      " Cleaned depression_comments.csv: 1378 → 1274 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_depression_comments.csv\n",
      "depression_help_comments.csv\n",
      "\n",
      " Cleaning: depression_help_comments.csv\n",
      " Cleaned depression_help_comments.csv: 2422 → 2325 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_depression_help_comments.csv\n",
      "depression_help_posts.csv\n",
      "\n",
      " Cleaning: depression_help_posts.csv\n",
      " Cleaned depression_help_posts.csv: 968 → 945 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_depression_help_posts.csv\n",
      "depression_posts.csv\n",
      "\n",
      " Cleaning: depression_posts.csv\n",
      " Cleaned depression_posts.csv: 983 → 969 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_depression_posts.csv\n",
      "mentalhealth_comments.csv\n",
      "\n",
      " Cleaning: mentalhealth_comments.csv\n",
      " Cleaned mentalhealth_comments.csv: 1972 → 1809 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_mentalhealth_comments.csv\n",
      "mentalhealth_posts.csv\n",
      "\n",
      " Cleaning: mentalhealth_posts.csv\n",
      " Cleaned mentalhealth_posts.csv: 961 → 948 entries\n",
      " Saved to: ../references/data_cleaned\\cleaned_mentalhealth_posts.csv\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# basic detection\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Includes all the best practices for text cleaning necessary for the main analysis: \n",
    "    - Lowercasing\n",
    "    - Removing URLs\n",
    "    - Removing brackets and their content\n",
    "    - Removing punctuation\n",
    "    - Tokenization\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)      \n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \"\", text)  \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "for file in os.listdir(data_folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(data_folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(file)\n",
    "        print(f\"\\n Cleaning: {file}\")\n",
    "\n",
    "        # Identify whether it's a post or comment file\n",
    "        if \"post\" in file:\n",
    "            text_col = \"selftext\"\n",
    "            id_col = \"post_id\"\n",
    "        elif \"comment\" in file:\n",
    "            text_col = \"body\"\n",
    "            id_col = \"comment_id\"\n",
    "        else:\n",
    "            print(\" Skipping unrecognized file:\", file)\n",
    "            continue\n",
    "\n",
    "        original_len = len(df)\n",
    "\n",
    "        #  Drop [deleted] and [removed]\n",
    "        df = df[~df[text_col].isin([\"[deleted]\", \"[removed]\"])]\n",
    "\n",
    "        #  Drop short/low-effort content (fewer than 5 words)\n",
    "        df = df[df[text_col].astype(str).str.split().str.len() >= 5]\n",
    "\n",
    "        # Keep only English-language texts\n",
    "        df = df[df[text_col].astype(str).apply(is_english)]\n",
    "\n",
    "        #  Remove duplicates based on ID column\n",
    "        df = df.drop_duplicates(subset=[id_col])\n",
    "\n",
    "        # apply cleaning function\n",
    "        df[\"clean_text\"] = df[text_col].apply(clean_text)\n",
    "\n",
    "        cleaned_len = len(df)\n",
    "        print(f\" Cleaned {file}: {original_len} → {cleaned_len} entries\")\n",
    "\n",
    "        # Save cleaned version\n",
    "        cleaned_path = os.path.join(cleaned_folder_path, f\"cleaned_{file}\")\n",
    "        df.to_csv(cleaned_path, index=False)\n",
    "        print(f\" Saved to: {cleaned_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
